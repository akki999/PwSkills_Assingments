{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6f58564",
   "metadata": {},
   "source": [
    "## Python Web Scraping Assignment:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8018bbec",
   "metadata": {},
   "source": [
    "Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9852cf7",
   "metadata": {},
   "source": [
    "A. Web Scraping:\n",
    "    \n",
    "    Web scraping is an automatic method to obtain large amounts of data from websites. \n",
    "    Most of this data is unstructured data in an HTML format which is then converted into structured data in a spreadsheet or a database so that it can be used in various applications. \n",
    "    There are many different ways to perform web scraping to obtain data from websites. \n",
    "    These include using online services, particular API’s or even creating your code for web scraping from scratch. \n",
    "\n",
    "B. Why is it used:\n",
    "    \n",
    "    Many large websites, like Google, Twitter, Facebook, StackOverflow, etc. have API’s that allow you to access their data in a structured format. \n",
    "    This is the best option, but there are other sites that don’t allow users to access large amounts of data in a structured form or they are simply not that technologically advanced. \n",
    "    In that situation, it’s best to use Web Scraping to scrape the website for data.\n",
    "    \n",
    "C. Give three areas where Web Scraping is used to get data.\n",
    "\n",
    "    Lead Generation for Marketing.\n",
    "    Price Comparison & Competition Monitoring\n",
    "    E-Commerce\n",
    "    Real Estate\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816f61cd",
   "metadata": {},
   "source": [
    "Q2. What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a412f1",
   "metadata": {},
   "source": [
    "Ans: Web Scrapers can be divided on the basis of many different criteria, including Self-built or Pre-built Web Scrapers, Browser extension or Software Web Scrapers, and Cloud or Local Web Scrapers.\n",
    "    \n",
    "    A. Self-built Web Scrapers:\n",
    "        \n",
    "        You can have Self-built Web Scrapers but that requires advanced knowledge of programming. \n",
    "        And if you want more features in your Web Scraper, then you need even more knowledge. \n",
    "        On the other hand, pre-built Web Scrapers are previously created scrapers that you can download and run easily. \n",
    "        These also have more advanced options that you can customize.\n",
    "        \n",
    "    B. Browser extensions Web Scrapers:\n",
    "        \n",
    "        Browser extensions Web Scrapers are extensions that can be added to your browser. \n",
    "        These are easy to run as they are integrated with your browser, but at the same time, they are also limited because of this. \n",
    "        Any advanced features that are outside the scope of your browser are impossible to run on Browser extension Web Scrapers. \n",
    "        But Software Web Scrapers don’t have these limitations as they can be downloaded and installed on your computer. \n",
    "        These are more complex than Browser web scrapers, but they also have advanced features that are not limited by the scope of your browser.\n",
    "        \n",
    "    C. Cloud Web Scrapers:\n",
    "        \n",
    "        Cloud Web Scrapers run on the cloud, which is an off-site server mostly provided by the company that you buy the scraper from. \n",
    "        These allow your computer to focus on other tasks as the computer resources are not required to scrape data from websites. \n",
    "        Local Web Scrapers, on the other hand, run on your computer using local resources. \n",
    "        So, if the Web scrapers require more CPU or RAM, then your computer will become slow and not be able to perform other tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeaa2de",
   "metadata": {},
   "source": [
    "Q3. What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e8abee",
   "metadata": {},
   "source": [
    "A. Beaautiful Soup:\n",
    "    \n",
    "    Beautiful Soup is a Python package for parsing HTML and XML documents (including having malformed markup, i.e. non-closed tags, so named after tag soup). \n",
    "    It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping.\n",
    "    \n",
    "B. Why is it used:\n",
    "    \n",
    "    Very fast\n",
    "    Extremely lenient\n",
    "    Parses pages the same way a Browser does\n",
    "    Prettify the Source Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374590f9",
   "metadata": {},
   "source": [
    "Q4. Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d0ca9b",
   "metadata": {},
   "source": [
    "Flask is a lightweight framework to build websites. We’ll use this to parse our collected data and display it as HTML in a new HTML file.\n",
    "\n",
    "The requests module allows us to send http requests to the website we want to scrape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af61cdd9",
   "metadata": {},
   "source": [
    "Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6be9ee",
   "metadata": {},
   "source": [
    "A. Code Pipeline and Beanstalk are aws services used in this project.\n",
    "\n",
    "B. i. Code Pipeline:\n",
    "    \n",
    "        AWS CodePipeline is a continuous delivery service you can use to model, visualize, and automate the steps required to release your software. \n",
    "        You can quickly model and configure the different stages of a software release process. \n",
    "        CodePipeline automates the steps required to release your software changes continuously.\n",
    "        \n",
    "   ii. Beanstalk:\n",
    "    \n",
    "        AWS Elastic Beanstalk automates the details of capacity provisioning, load balancing, auto scaling, and application deployment, creating an environment that runs a version of your application. \n",
    "        You can simply upload your deployable code (e.g., WAR file), and AWS Elastic Beanstalk does the rest.\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674f41e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
